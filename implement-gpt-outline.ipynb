{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#imports\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:30.992441Z","iopub.execute_input":"2023-09-24T07:17:30.992974Z","iopub.status.idle":"2023-09-24T07:17:30.999634Z","shell.execute_reply.started":"2023-09-24T07:17:30.992932Z","shell.execute_reply":"2023-09-24T07:17:30.997909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#download the tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:31.002946Z","iopub.execute_input":"2023-09-24T07:17:31.003374Z","iopub.status.idle":"2023-09-24T07:17:32.404787Z","shell.execute_reply.started":"2023-09-24T07:17:31.003340Z","shell.execute_reply":"2023-09-24T07:17:32.403211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read the text data into the variable 'text'\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.407690Z","iopub.execute_input":"2023-09-24T07:17:32.408231Z","iopub.status.idle":"2023-09-24T07:17:32.415343Z","shell.execute_reply.started":"2023-09-24T07:17:32.408174Z","shell.execute_reply":"2023-09-24T07:17:32.414134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#see how long the dataset is\nprint(len(text))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.416935Z","iopub.execute_input":"2023-09-24T07:17:32.417357Z","iopub.status.idle":"2023-09-24T07:17:32.437182Z","shell.execute_reply.started":"2023-09-24T07:17:32.417318Z","shell.execute_reply":"2023-09-24T07:17:32.436231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#see the first 1000 characters of text\nprint(text[:1000])","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.439683Z","iopub.execute_input":"2023-09-24T07:17:32.441046Z","iopub.status.idle":"2023-09-24T07:17:32.452292Z","shell.execute_reply.started":"2023-09-24T07:17:32.441007Z","shell.execute_reply":"2023-09-24T07:17:32.450544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the set of characters in the dataset followed by the size of the set\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.454455Z","iopub.execute_input":"2023-09-24T07:17:32.455087Z","iopub.status.idle":"2023-09-24T07:17:32.487052Z","shell.execute_reply.started":"2023-09-24T07:17:32.455042Z","shell.execute_reply":"2023-09-24T07:17:32.485821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create the character to integer mapping that will be necessary in tokenization\nstoi = { ch:i for i, ch in enumerate(chars)}\nitos = { i:ch for i, ch in enumerate(chars)}\n\nencode = lambda s: [stoi[c] for c in s] #encoder- takes a string then outputs a list of ints\ndecode = lambda l: ''.join([itos[i] for i in l]) #decoder- take the list of ints and output a string","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.488732Z","iopub.execute_input":"2023-09-24T07:17:32.490543Z","iopub.status.idle":"2023-09-24T07:17:32.501968Z","shell.execute_reply.started":"2023-09-24T07:17:32.490501Z","shell.execute_reply":"2023-09-24T07:17:32.500602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test encode and decode\nprint(encode(\"i love transformers!\"))\nprint(decode(encode(\"i love transformers!\")))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.503390Z","iopub.execute_input":"2023-09-24T07:17:32.503712Z","iopub.status.idle":"2023-09-24T07:17:32.515956Z","shell.execute_reply.started":"2023-09-24T07:17:32.503684Z","shell.execute_reply":"2023-09-24T07:17:32.514788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encode the text and store it in a torch.Tensor\ndata = torch.tensor(encode(text), dtype = torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) #the original dataset rerepresented as a sequence of integers","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.517547Z","iopub.execute_input":"2023-09-24T07:17:32.517999Z","iopub.status.idle":"2023-09-24T07:17:32.768750Z","shell.execute_reply.started":"2023-09-24T07:17:32.517958Z","shell.execute_reply":"2023-09-24T07:17:32.767621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split the dataset into training and validation sets\nn = int(0.9*len(data)) #90% train, 10% val\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.770395Z","iopub.execute_input":"2023-09-24T07:17:32.771118Z","iopub.status.idle":"2023-09-24T07:17:32.777370Z","shell.execute_reply.started":"2023-09-24T07:17:32.771075Z","shell.execute_reply":"2023-09-24T07:17:32.776339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set the context window to train on\nblock_size = 8\ntrain_data[:block_size+1]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.780929Z","iopub.execute_input":"2023-09-24T07:17:32.781965Z","iopub.status.idle":"2023-09-24T07:17:32.794601Z","shell.execute_reply.started":"2023-09-24T07:17:32.781927Z","shell.execute_reply":"2023-09-24T07:17:32.793537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create the context window that each batch will train on\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\n\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.796129Z","iopub.execute_input":"2023-09-24T07:17:32.796577Z","iopub.status.idle":"2023-09-24T07:17:32.809135Z","shell.execute_reply.started":"2023-09-24T07:17:32.796535Z","shell.execute_reply":"2023-09-24T07:17:32.807941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(1337)\nbatch_size = 4 #the number of independent sequences that will process in parallel\nblock_size = 8 #the maximum context length for predictions\n\ndef get_batch(split):\n    #generate a small batch of data of inputs (x) and targets (y)\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs')\nprint(xb.shape)\nprint(xb)\nprint('targets')\nprint(yb.shape)\nprint(yb)\n\nprint(\"\")\n\nfor b in range(batch_size): #batch dimension\n    for t in range(block_size): #time dimension\n        context = xb[b,:t+1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target is: {target}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.813282Z","iopub.execute_input":"2023-09-24T07:17:32.813624Z","iopub.status.idle":"2023-09-24T07:17:32.832210Z","shell.execute_reply.started":"2023-09-24T07:17:32.813597Z","shell.execute_reply":"2023-09-24T07:17:32.830841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    \n    def __init__(self, vocab_size):\n        super().__init__()\n        #each token reads off the logits for the next token using a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        #idc and targets are both (Batch,Time) tensor of integers\n        logits = self.token_embedding_table(idx) #(Batch,Time,Channel)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n        \n    def generate(self, idx, max_new_tokens):\n        #idx is the (Batch, Time) array of indices in the current context\n        for _ in range(max_new_tokens):\n            #take the predictions\n            logits, loss = self(idx)\n            #focus only on the last time step\n            logits = logits[:, -1, :] #becomes (Batch, Channel)\n            #softmax values\n            probs = F.softmax(logits, dim = 1) #(Batch, Channel)\n            #sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) #(Batch, 1)\n            #append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) #(Batch, Time+1)\n        return idx\n        \nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\n\nprint(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens = 100)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.834205Z","iopub.execute_input":"2023-09-24T07:17:32.835339Z","iopub.status.idle":"2023-09-24T07:17:32.870444Z","shell.execute_reply.started":"2023-09-24T07:17:32.835292Z","shell.execute_reply":"2023-09-24T07:17:32.869124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a PyTorch optimizer object\noptimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.871921Z","iopub.execute_input":"2023-09-24T07:17:32.872295Z","iopub.status.idle":"2023-09-24T07:17:32.878249Z","shell.execute_reply.started":"2023-09-24T07:17:32.872262Z","shell.execute_reply":"2023-09-24T07:17:32.877145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nfor steps in range(1000):\n    \n    #sample a batch of data\n    xb, yb = get_batch('train')\n    \n    #evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    \nprint(loss.item())","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:32.879754Z","iopub.execute_input":"2023-09-24T07:17:32.880134Z","iopub.status.idle":"2023-09-24T07:17:34.426666Z","shell.execute_reply.started":"2023-09-24T07:17:32.880104Z","shell.execute_reply":"2023-09-24T07:17:34.424691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#see what kind of text we can generate\nprint(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens = 300)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:34.428822Z","iopub.execute_input":"2023-09-24T07:17:34.429258Z","iopub.status.idle":"2023-09-24T07:17:34.472903Z","shell.execute_reply.started":"2023-09-24T07:17:34.429218Z","shell.execute_reply":"2023-09-24T07:17:34.471822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mathematical trick in self-attention","metadata":{}},{"cell_type":"code","source":"#consider the following example\n\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2 #batch, time, channels\nx = torch.randn(B,T,C)\nx.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:34.474536Z","iopub.execute_input":"2023-09-24T07:17:34.474897Z","iopub.status.idle":"2023-09-24T07:17:34.483449Z","shell.execute_reply.started":"2023-09-24T07:17:34.474865Z","shell.execute_reply":"2023-09-24T07:17:34.482551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#version 1\n#we want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C)) #bow = bag of words\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] #(t, C)\n        xbow[b,t] = torch.mean(xprev, 0) \n        #this method is slow and inefficient, we will instead use matrix multiplication ","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:34.484672Z","iopub.execute_input":"2023-09-24T07:17:34.485716Z","iopub.status.idle":"2023-09-24T07:17:34.497149Z","shell.execute_reply.started":"2023-09-24T07:17:34.485674Z","shell.execute_reply":"2023-09-24T07:17:34.496002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x[0])\nprint(xbow[0])","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:34.498403Z","iopub.execute_input":"2023-09-24T07:17:34.498727Z","iopub.status.idle":"2023-09-24T07:17:34.508173Z","shell.execute_reply.started":"2023-09-24T07:17:34.498701Z","shell.execute_reply":"2023-09-24T07:17:34.507069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#version 2\n#perform weighted sums in a triangular form, token T will only \n#get information from other tokens preceding it\nwei = torch.tril(torch.ones(T,T))\nwei = wei / wei.sum(1, keepdim = True)\nxbow2 = wei @ x #(B, T,T) @ (B, T, C) -> (B, T, C)\ntorch.allclose(xbow, xbow2)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:34.509274Z","iopub.execute_input":"2023-09-24T07:17:34.509584Z","iopub.status.idle":"2023-09-24T07:17:34.521103Z","shell.execute_reply.started":"2023-09-24T07:17:34.509556Z","shell.execute_reply":"2023-09-24T07:17:34.520176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#version 3: using Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:34.522510Z","iopub.execute_input":"2023-09-24T07:17:34.522826Z","iopub.status.idle":"2023-09-24T07:17:34.532657Z","shell.execute_reply.started":"2023-09-24T07:17:34.522800Z","shell.execute_reply":"2023-09-24T07:17:34.531561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0,10,(3,2)).float()\nc = a @ b #matrix multiplication\nprint('a =', a)\nprint('b =', b)\nprint('c =', c)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:34.534159Z","iopub.execute_input":"2023-09-24T07:17:34.534555Z","iopub.status.idle":"2023-09-24T07:17:34.545694Z","shell.execute_reply.started":"2023-09-24T07:17:34.534527Z","shell.execute_reply":"2023-09-24T07:17:34.544570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#version 4: self-attention\ntorch.manual_seed(1337)\nB,T,C = 4, 8, 32 #batch, time, channels\nx = torch.randn(B,T,C)\n\n#single head performs self attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x) #B,T,16\nq = query(x) #B,T,16\n\n#make sure wei has variance 1, and not variance = batch_size\nwei = q @ k.transpose(-2, -1)* head_size**-0.5 # B,T,16 @ B,16,T -> B,T,T\n\n\ntril = torch.tril(torch.ones(T,T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x #x is private to a v\n\nout.shape\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:34.547157Z","iopub.execute_input":"2023-09-24T07:17:34.547695Z","iopub.status.idle":"2023-09-24T07:17:34.560683Z","shell.execute_reply.started":"2023-09-24T07:17:34.547665Z","shell.execute_reply":"2023-09-24T07:17:34.559557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" wei[0]\n#No longer uniform lower triangular matrices, rather the weights are different on a\n#batch by batch basis","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:34.562391Z","iopub.execute_input":"2023-09-24T07:17:34.562720Z","iopub.status.idle":"2023-09-24T07:17:34.578073Z","shell.execute_reply.started":"2023-09-24T07:17:34.562692Z","shell.execute_reply":"2023-09-24T07:17:34.577051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LayerNorm1d: #used to be BatchNorm1d\n  \n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n  \n  def __call__(self, x):\n    #calculate the forward pass\n    xmean = x.mean(1, keepdim=True) #calculate batch mean\n    xvar = x.var(1, keepdim=True) #calculate batch variance\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) #normalize to variance = 1\n    self.out = self.gamma * xhat + self.beta\n    return self.out\n  \n  def parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100)\nx = torch.randn(32, 100) #batch_size = 32 of 100-dim vectors\nx = module(x)\nx.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:17:34.579198Z","iopub.execute_input":"2023-09-24T07:17:34.579985Z","iopub.status.idle":"2023-09-24T07:17:34.594382Z","shell.execute_reply.started":"2023-09-24T07:17:34.579952Z","shell.execute_reply":"2023-09-24T07:17:34.593108Z"},"trusted":true},"execution_count":null,"outputs":[]}]}